{
  "hash": "9415ece84fb26d2731f2958934e2ae04",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: '\"Controllare per\" ma dimenticare le cause comuni (i.e., \"latenti\")'\nauthor: \"Enrico Toffalini\"\ndate: \"2025-12-09\"\nbibliography: \"references.bib\"\nformat: html\n---\n\n::: abstract\n**Riassunto: Non basta controllare per tante cose, serve un modello di misura e un vero DAG per l'inferenza causale. E i SEM sono lo strumento per implementarli e per testare davvero le teorie.**\n:::\n\n*(Questi appunti, ancora da riordinare sono stimolati da un recente e utilissimo <a href=\"https://feracotommaso.github.io/posts/iatmeasure.html\" target=\"_blank\">post di Tommaso Feraco</a> e da numerose cose viste negli anni, nonch√© dai miei stessi errori.)*\n\nPoniamo di vedere in un paper una cosa tipo quella sotto. √à un caso paradigmatico. Il paper racconta che diverse abilit√† cognitive predicono l'outcome (es. l'adattamento), ciascuna indipendentemente e *above and beyond* le altre. Assumendo che i fattori cognitivi siano causali rispetto all'adattamento, si pu√≤ anche dire che *ciascuna abilit√† cognitiva causa l'outcome al netto delle altre*. \n\n<figure>\n  <img src=\"wrong-model.png\" alt=\"wrong-model\" style=\"width: 100%; height: auto;\">\n  <figcaption><b>Figura 1</b>. <em>Diverse abilit√† cognitive sembrano predire indipendentemente lo stesso outcome</em></figcaption>\n</figure>\n\nIn realt√†, nessuna delle abilit√† cognitive causa necessariamente l'adattamento, nemmeno sotto l'assunzione fatta. Perch√©? Perch√© probabilmente, almeno secondo i modelli ora mainstream, √® un errore di inferenza causale. \n\nI predittori (abilit√† verbali, ragionamento fluido, ecc.) evidentemente correlano tra loro: mostrano una *positive manifold*. ***Ma perch√© correlano?*** Questo √® il problema. Sebbene esistano diverse teorie dell'intelligenza, la visione tuttora mainstream √® che diverse abilit√† correlano perch√© condividono delle ***cause comuni***. Queste cause comuni sono spesso definite \"fattore generale\" (g) dell'intelligenza. Si pu√≤ opinare, si pu√≤ dire che in tutto o in parte alcune abilit√† siano a loro volta causali rispetto ad altre (es. alcuni danno forte centralit√† alla memoria di lavoro/working memory, ritenendola a monte di altre abilit√†). Va bene: in qualsiasi caso, per fare inferenza valida dovremmo partire da un DAG delle relazioni causali, in cui non ci si limita a dire \"le cose correlano\", ma si cerca di dare conto del perch√©. \n\nSe prendiamo il caso pi√π semplice, quello mainstream, assunto dai modelli gerarchici delle abilit√† cognitive quali il CHC, possiamo assumere che le diverse abilit√† cognitive correlino anche *solo* perch√© condividono cause comuni (il fattore g). Se implementiamo questo modello correttamente, senza dover raccogliere nessuno dato in pi√π, usando solo quello che abbiamo gi√† nel dataset, potremmo scoprire che ***tutti*** gli effetti precedentemente \"scoperti\" spariscono, si azzerano, non sono pi√π significativi. \n\n<figure>\n  <img src=\"right-model.png\" alt=\"right-model\" style=\"width: 100%; height: auto;\">\n  <figcaption><b>Figura 2</b>. <em>Una volta aggiunto un fattore sovraordinato che spiega la positive manifold tra i predittori assumendo cause comuni tra essi, potremmo scoprire che √® tale fattore che spiega l'outcome, non i fattori specifici</em></figcaption>\n</figure>\n\nPer gli utenti di R che conoscono il pacchetto `lavaan`, qui sotto c'√® del codice R che simula e riproduce la situazione illustrata sopra (non √® possibile fittare un modello che abbia contemporaneamente *g* e anche tutti i fattori cognitivi come predittori dell'outcome, ma si possono testare uno alla volta o a blocchi) üëá *Enjoy it!*\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Clicca qui per vedere il codice R\"}\nlibrary(lavaan)\n\n# set seed per riproducibility\nset.seed(0)\n\n# genera dati e metti in un dataframe\nN = 10000\ng = rnorm(N,0,1)\nVerbal = g*0.7 + rnorm(N,0,0.51)\nVisual = g*0.6 + rnorm(N,0,0.64)\nMemory = g*0.7 + rnorm(N,0,0.51)\nFluid = g*0.8 + rnorm(N,0,0.36)\n\nAdaptiveBehavior = g*0.5 + rnorm(N,0,1)\n\ndf = data.frame(verb1 = Verbal + rnorm(N,0,0.7),\n                verb2 = Verbal + rnorm(N,0,0.7),\n                verb3 = Verbal + rnorm(N,0,0.7),\n                vis1 = Visual + rnorm(N,0,0.7),\n                vis2 = Visual + rnorm(N,0,0.7),\n                vis3 = Visual + rnorm(N,0,0.7),\n                mem1 = Memory + rnorm(N,0,0.7),\n                mem2 = Memory + rnorm(N,0,0.7),\n                fluid1 = Fluid + rnorm(N,0,0.7),\n                fluid2 = Fluid + rnorm(N,0,0.7),\n                fluid3 = Fluid + rnorm(N,0,0.7),\n                adapt1 = AdaptiveBehavior + rnorm(N,0,0.7),\n                adapt2 = AdaptiveBehavior + rnorm(N,0,0.7),\n                adapt3 = AdaptiveBehavior + rnorm(N,0,0.7)\n)\n\n#### Fittiamo il Modello Sbagliato\nwrong_model = \"\nVerbal_factor =~ verb1 + verb2 + verb3\nVisual_factor =~ vis1 + vis2 + vis3\nMemory_factor =~ mem1 + mem2\nFluid_factor =~ fluid1 + fluid2 + fluid3\n\nAdaptive_factor =~ adapt1 + adapt2 + adapt3\n\nAdaptive_factor ~ Verbal_factor + Visual_factor + Memory_factor + Fluid_factor\n\"\nfitWrong = sem(model=wrong_model, data=df, std.lv=T)\nsummary(fitWrong)\n\n\n#### Fittiamo il Modello Giusto\nok_model = \"\nVerbal_factor =~ verb1 + verb2 + verb3\nVisual_factor =~ vis1 + vis2 + vis3\nMemory_factor =~ mem1 + mem2\nFluid_factor =~ fluid1 + fluid2 + fluid3\n\ng =~ Verbal_factor + Visual_factor + Memory_factor + Fluid_factor\n\nAdaptive_factor =~ adapt1 + adapt2 + adapt3\n\nAdaptive_factor ~ g + Verbal_factor + Visual_factor + Fluid_factor\n\"\nfitOk = sem(model=ok_model, data=df, std.lv=T)\nsummary(fitOk)\n```\n:::\n\n\n## Alcuni altri appunti pratici \n\n1. Dire che le abilit√† cognitive *\"sono fattori distinti\"* per evitare il fattore sovraordinato elude il problema teorico, non considera l'esistenza di cause comuni, e comunque non spiega le intercorrelazioni. S√¨, alla fine √® tutto un problema di inferenza causale e di DAG, che sono cose che s'insegnano troppo poco all'universit√†.\n1. Spesso errori concettuali come quelli illustrati sopra si sono visti usare per accreditare il ruolo delle \"*Funzioni Esecutive*\" al di sopra della \"*Intelligenza*\" rispetto all'*Adattamento*, senza considerare che in quanto abilit√† cognitive anche le *Funzioni Esecutive* sono teoricamente sottoposte alle cause comuni delle abilit√† cognitive (g); la cosa andrebbe comunque testata seriamente. \n1. Spesso si considera che controllare per \"Ragionamento fluido\" / intelligenza fluida sia sufficiente e paragonabile a \"controllare per intelligenza generale\": anch'io sono caduto spesso in questo equivoco; ma no, \"Ragionamento fluido\" **non** √® uguale a g.\n1. Di solito la situazione √® anche peggiore di come mostrato sopra, perch√© ci si limita a variabili osservate, senza nemmeno ricorrere ai SEM. \n1. Anche se avete un solo indicatore per abilit√† cognitiva, potete fare un sem lo stesso, dovendo per√≤ forzare quanta componente di abilit√† e quanta di rumore ci sia nell'indicatore osservato.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}